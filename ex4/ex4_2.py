#Back propagation
#å…ˆå‰å‘ä¼ æ’­
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.io import loadmat

data=loadmat("ex4data1.mat")#è¿™é‡Œçš„æ•°æ®å’Œex3data1ç›¸åŒ
X=data['X']
y=data['y']
# print(X.shape,y.shape)#(5000, 400) (5000, 1)

from sklearn.preprocessing import OneHotEncoder
encoder=OneHotEncoder(sparse=False)
y_onehot=encoder.fit_transform(y)
# print(y_onehot.shape)#(5000, 10) è¯´æ˜æ•°æ®åªæœ‰10ç±»

def sigmod(z):
    return 1 / (1+np.exp(-z))

#å‰å‘ä¼ æ’­å‡½æ•°ï¼šï¼ˆ400+1ï¼‰->ï¼ˆ25+1ï¼‰->ï¼ˆ10ï¼‰ï¼Œè¿™é‡Œä¸­é—´å±‚ä¸ºä¸€å±‚
def forward_propagate(X,theta1,theta2):
    m=X.shape[0]
    a1=np.insert(X,0,values=np.ones(m),axis=1)
    z2=a1*theta1.T
    a2=np.insert(sigmod(z2),0,np.ones(m),axis=1)
    z3=a2*theta2.T
    h=sigmod(z3)
    return a1,z2,a2,z3,h

def cost(params,input_size,hidden_size,num_labels,X,y,alpha):
    m=X.shape[0]
    X=np.matrix(X)
    y=np.matrix(y)
    # å°†å‚æ•°æ•°ç»„æ­å¼€ä¸ºæ¯å±‚çš„å‚æ•°çŸ©é˜µ
    theta1=np.matrix(np.reshape(params[:hidden_size*(input_size+1)],(hidden_size,(input_size+1))))
    theta2=np.matrix(np.reshape(params[hidden_size*(input_size+1):],(num_labels,(hidden_size+1))))
    '''
    ğ‘ğ‘–(ğ‘—) ä»£è¡¨ç¬¬ç¬¬ ğ‘— å±‚çš„ç¬¬ ğ‘– ä¸ªæ¿€æ´»å•å…ƒæˆ–è€…ç”¨slã€‚
     ğœƒ(ğ‘—)ä»£è¡¨ä»ç¬¬ ğ‘— å±‚æ˜ å°„åˆ°ç¬¬ ğ‘—+1 å±‚æ—¶çš„æƒé‡çŸ©,ä¾‹å¦‚ ğœƒ(1)ä»£è¡¨ä»ç¬¬ä¸€å±‚æ˜ å°„åˆ°ç¬¬äºŒå±‚çš„æƒé‡çŸ©é˜µã€‚
     å…¶å°ºå¯¸ä¸ºï¼šç¬¬ ğ‘—+1 å±‚çš„æ¿€æ´»å•å…ƒæ•°é‡ä¸ºè¡Œï¼Œä»¥ç¬¬ ğ‘— å±‚çš„æ¿€æ´»å•å…ƒæ•°åŠ ä¸€ä¸ºåˆ—çŸ©é˜µã€‚
     '''
    a1, z2, a2, z3, h=forward_propagate(X,theta1,theta2)
    J=0
    for i in range(m):
        first_term=np.multiply(-y[i,:],np.log(h[i,:]))
        second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))
        J+=np.sum(first_term-second_term)
    return J/m

#åˆå§‹åŒ–å‚æ•°
input_size=400
hidden_size=25
num_labels=10
alpha=1
#éšæœºåˆå§‹åŒ–å®Œæˆç½‘ç»œå‚æ•°å¤§å°çš„å‚æ•°æ•°ç»„
params=(np.random.random(size=hidden_size*(input_size+1) + num_labels*(hidden_size+1)) - 0.5) *0.25
m=X.shape[0]
X=np.matrix(X)
y=np.matrix(y)
#å°†å‚æ•°æ•°ç»„æ­å¼€ä¸ºæ¯å±‚çš„å‚æ•°çŸ©é˜µ
theta1=np.matrix(np.reshape(params[:hidden_size*(input_size+1)],(hidden_size,(input_size+1))))
theta2=np.matrix(np.reshape(params[hidden_size*(input_size+1):],(num_labels,(hidden_size+1))))
# print(theta1.shape,theta2.shape)# (25, 401) (10, 26)oo
a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)
# print(a1.shape,z2.shape,a2.shape,z3.shape,h.shape) #(5000, 401) (5000, 25) (5000, 26) (5000, 10) (5000, 10)

#è¾“å‡ºå‡è®¾hå’ŒçœŸå®yçš„è¯¯å·®ä¹‹å’Œï¼ˆæ€»è¯¯å·®ï¼‰
# print(cost(params,input_size,hidden_size,num_labels,X,y_onehot,alpha))

def costReg(params,input_size,hidden_size,num_labels,X,y,alpha):
    m=X.shape[0]
    X=np.matrix(X)
    y=np.matrix(y)
    # å°†å‚æ•°æ•°ç»„æ­å¼€ä¸ºæ¯å±‚çš„å‚æ•°çŸ©é˜µ
    theta1=np.matrix(np.reshape(params[:hidden_size*(input_size+1)],(hidden_size,(input_size+1))))
    theta2=np.matrix(np.reshape(params[hidden_size*(input_size+1):],(num_labels,(hidden_size+1))))
    '''ğ‘ğ‘–(ğ‘—) ä»£è¡¨ç¬¬ç¬¬ ğ‘— å±‚çš„ç¬¬ ğ‘– ä¸ªæ¿€æ´»å•å…ƒæˆ–è€…ç”¨slã€‚
     ğœƒ(ğ‘—)ä»£è¡¨ä»ç¬¬ ğ‘— å±‚æ˜ å°„åˆ°ç¬¬ ğ‘—+1 å±‚æ—¶çš„æƒé‡çŸ©,ä¾‹å¦‚ ğœƒ(1)ä»£è¡¨ä»ç¬¬ä¸€å±‚æ˜ å°„åˆ°ç¬¬äºŒçš„å±‚æƒé‡çŸ©é˜µã€‚
     å…¶å°ºå¯¸ä¸ºï¼šç¬¬ğ‘—+1å±‚çš„æ¿€æ´»å•å…ƒæ•°é‡ä¸ºè¡Œï¼Œä»¥ç¬¬ ğ‘— å±‚çš„æ¿€æ´»å•å…ƒæ•°åŠ ä¸€ä¸ºåˆ—çŸ©é˜µã€‚
     '''
    a1, z2, a2, z3, h=forward_propagate(X,theta1,theta2)
    J=0
    for i in range(m):
        first_term=np.multiply(-y[i,:],np.log(h[i,:]))
        second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))
        J+=np.sum(first_term-second_term)

    #å¢åŠ æ­£åˆ™é¡¹
    reg=float(alpha)/2/m*(np.sum(np.power(theta1[:,1:],2))+np.sum(np.power(theta2[:,1:],2)))
    return J/m+reg


#åå‘ä¼ æ’­å¼€å§‹
def sigmod_gradient(z): #sigmodå‡½æ•°çš„æ±‚å¯¼
    return np.multiply(sigmod(z),(1-sigmod(z)))

def back_propagate_without_reg(params,input_size,hidden_size,num_labels,X,y,alpha):
    m=X.shape[0]
    X=np.matrix(X)
    y=np.matrix(y)
    # å°†å‚æ•°æ•°ç»„æ­å¼€ä¸ºæ¯å±‚çš„å‚æ•°çŸ©é˜µ
    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))
    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))
    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)

    #åˆå§‹åŒ–
    J=0
    delta1=np.zeros(theta1.shape)
    delta2=np.zeros(theta2.shape)
    #è®¡ç®—cost
    for i in range(m):
        first_term=np.multiply(-y[i,:],np.log(h[i,:]))
        second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))
        J+=np.sum(first_term-second_term)
    reg = float(alpha) / 2 / m * (np.sum(np.power(theta1[:, 1 :], 2)) + np.sum(np.power(theta2[:, 1:], 2)))
    J=J/m+reg

    #åå‘ä¼ æ’­å¼€å§‹
    for t in range(m): #æ¯è¡Œè®¡ç®—ä¸€æ¬¡ï¼ˆä¹Ÿå°±æ˜¯æ¯å¯¹æ•°æ®ï¼‰
        a1t=a1[t,:]#(1,401)
        z2t=z2[t,:]#(1,25)
        a2t=a2[t,:]#(1,26)
        ht=h[t,:]#(1,10)
        yt=y[t,:]#(1,10)
        d3t=ht-yt#(1,10)#
        z2t=np.insert(z2t,0,values=np.ones(1))#(1,26)#å…¶å®æ“ä½œå®Œä¹‹åå°±æ˜¯a2t
        d2t=np.multiply((theta2.T * d3t.T).T,sigmod_gradient(z2t))#(1,26)
        delta1=delta1+(d2t[:,1:]).T*a1t
        delta2=delta2+d3t.T*a2t

    delta1=delta1/m
    delta2=delta2/m

    grad=np.concatenate((np.ravel(delta1),np.ravel(delta2)))
    return J,grad

J,grad=back_propagate_without_reg(params,input_size,hidden_size,num_labels,X,y_onehot,alpha)
# print(J,grad.shape)

def back_propagate_with_reg(params,input_size,hidden_size,num_labels,X,y,alpha):
    m=X.shape[0]
    X=np.matrix(X)
    y=np.matrix(y)
    # å°†å‚æ•°æ•°ç»„æ­å¼€ä¸ºæ¯å±‚çš„å‚æ•°çŸ©é˜µ
    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))
    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))
    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)

    #åˆå§‹åŒ–
    J=0
    delta1=np.zeros(theta1.shape)
    delta2=np.zeros(theta2.shape)
    #è®¡ç®—cost
    for i in range(m):
        first_term=np.multiply(-y[i,:],np.log(h[i,:]))
        second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))
        J+=np.sum(first_term-second_term)
    reg = float(alpha) / 2 / m * (np.sum(np.power(theta1[:, 1 :], 2)) + np.sum(np.power(theta2[:, 1:], 2)))
    J=J/m+reg

    #åå‘ä¼ æ’­å¼€å§‹
    for t in range(m): #æ¯è¡Œè®¡ç®—ä¸€æ¬¡ï¼ˆä¹Ÿå°±æ˜¯æ¯å¯¹æ•°æ®ï¼‰
        a1t=a1[t,:]#(1,401)
        z2t=z2[t,:]#(1,25)
        a2t=a2[t,:]#(1,26)
        ht=h[t,:]#(1,10)
        yt=y[t,:]#(1,10)
        d3t=ht-yt#(1,10)
        z2t=np.insert(z2t,0,values=np.ones(1))#(1,26)#å…¶å®æ“ä½œå®Œä¹‹åå°±æ˜¯a2t
        d2t=np.multiply((theta2.T * d3t.T).T,sigmod_gradient(z2t))#(1,26)
        delta1=delta1+(d2t[:,1:]).T*a1t
        delta2=delta2+d3t.T*a2t

    delta1=delta1/m
    delta2=delta2/m

    #å¢åŠ æ­£åˆ™é¡¹ï¼šåªä¼šå½±å“grad
    delta1[:, 1:] = delta1[:, 1:] + (theta1[:, 1:] * alpha) / m
    delta2[:, 1:] = delta2[:, 1:] + (theta2[:, 1:] * alpha) / m

    grad=np.concatenate((np.ravel(delta1),np.ravel(delta2)))
    return J,grad
J_Reg,grad_Reg=back_propagate_with_reg(params,input_size,hidden_size,num_labels,X,y_onehot,alpha)
# print(J_Reg,grad_Reg.shape)

#å‡†å¤‡è®­ç»ƒç½‘ç»œ
from scipy.optimize import minimize
fmin=minimize(fun=back_propagate_with_reg,x0=params,args=(input_size,hidden_size,num_labels,X,y_onehot,alpha),method='TNC',jac=True,options={'maxiter':250})
print(fmin)

#å¼€å§‹é¢„æµ‹:ç±»ä¼¼äºé€»è¾‘å›å½’
X=np.matrix(X)
theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))
theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))
y_pre=np.argmax(h,axis=1)+1
print(y_pre)

#è®¡ç®—ç²¾å‡†åº¦
correct=[1 if a ==b else 0 for (a,b) in zip(y_pre,y)]
accuracy=sum(map(int,correct)) / float(len(correct))#è®¡ç®—1çš„ä¸ªæ•°å¹¶æ±‚ç™¾åˆ†æ¯”å³ç²¾ç¡®åº¦
print(accuracy)
